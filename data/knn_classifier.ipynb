{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors Classifier\n",
    "\n",
    "In this notebook, you will implement your own k-nearest neighbors (k-NN) algorithm for the classification problem. You are supposed to learn:\n",
    "\n",
    "* How to prepare the dataset for \"training\" and testing of the model.\n",
    "* How to implement k-nearest neighbors classification algorithm.\n",
    "* How to evaluate the performance of your classifier.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Read carefuly through this notebook. Be sure you understand what is provided to you, and what is required from you.\n",
    "* Place your code only in sections annotated with `### START CODE HERE ###` and `### END CODE HERE ###`.\n",
    "* Use comments whenever the code is not self-explanatory.\n",
    "* Submit an executable notebook (`*.ipynb`) with your solution to BlackBoard.\n",
    "\n",
    "Enjoy :-)\n",
    "\n",
    "## Packages\n",
    "\n",
    "Following packages is all you need. Do not import any additional packages!\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/) is a library providing easy-to-use data structures and data analysis tools.\n",
    "* [Numpy](http://www.numpy.org/) library provides support for large multi-dimensional arrays and matrices, along with functions to operate on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "You are given a dataset `mushrooms.csv` with characteristics/attributes of mushrooms, and your task is to implement and evaluate a k-nearest neighbors classifier able to say whether a mushroom is poisonous or edible based on its attributes.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset of mushroom characteristics is freely available at [Kaggle Datasets](https://www.kaggle.com/uciml/mushroom-classification) where you can find further information about the dataset. It consists of 8124 mushrooms characterized by 23 attributes (including the class). Following is the overview of attributes and values:\n",
    "\n",
    "* class: edible=e, poisonous=p\n",
    "* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n",
    "* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "* bruises: bruises=t,no=f\n",
    "* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n",
    "* gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "* gill-spacing: close=c,crowded=w,distant=d\n",
    "* gill-size: broad=b,narrow=n\n",
    "* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "* stalk-shape: enlarging=e,tapering=t\n",
    "* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "* veil-type: partial=p,universal=u\n",
    "* veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "* ring-number: none=n,one=o,two=t\n",
    "* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "Let's load the dataset into so called Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mushrooms_df = pd.read_csv('mushrooms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mushrooms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print an overview of all attributes with the counts of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushrooms_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is pretty much balanced. That's a good news for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "As our dataset consist of nominal/categorical values only, we will encode the strings into integers which will allow us to use similiraty measures such as Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df):\n",
    "    import sklearn.preprocessing\n",
    "    encoder = {}\n",
    "    for col in df.columns:\n",
    "        le = sklearn.preprocessing.LabelEncoder()\n",
    "        le.fit(df[col])\n",
    "        df[col] = le.transform(df[col])\n",
    "        encoder[col] = le\n",
    "    return df, encoder    \n",
    "\n",
    "mushrooms_encoded_df, encoder = encode_labels(mushrooms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mushrooms_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Before we start with the implementation of our k-nearest neighbors algorithm we need to prepare our dataset for the \"training\" and testing.\n",
    "\n",
    "First, we divide the dataset into attributes (often called features) and classes (often called targets). Keeping attributes and classes separately is a common practice in many implementations. This should simplify the implementation and make the code understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = mushrooms_encoded_df.drop('class', axis=1)  # attributes\n",
    "y_df = mushrooms_encoded_df['class']  # classes\n",
    "X_array = X_df.as_matrix()\n",
    "y_array = y_df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X =', X_array)\n",
    "print('y =', y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split the attributes and classes into training sets and test sets.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Implement the holdout splitting method with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Shuffles the dataset and splits it into training and test sets.\n",
    "    \n",
    "    :param X\n",
    "        attributes\n",
    "    :param y\n",
    "        classes\n",
    "    :param test_size\n",
    "        float between 0.0 and 1.0 representing the proportion of the dataset to include in the test split\n",
    "    :return\n",
    "        train-test splits (X-train, X-test, y-train, y-test)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation/test set with 67:33 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('X_train =', X_train)\n",
    "print('y_train =', y_train)\n",
    "print('X_test =', X_test)\n",
    "print('y_test =', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(y_train) == 5443\n",
    "assert len(X_test) == len(y_test)\n",
    "assert len(y_test) == 2681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The k-nearest neighbors algorithm doesn't require a training step. The class of an unseen sample is deduced by comparison with samples of known class.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Implement the k-nearest neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section to place any \"helper\" code for the `knn()` function.\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_true, y_true, X_pred, k=5):\n",
    "    \"\"\"\n",
    "    k-nearest neighbors classifier.\n",
    "    \n",
    "    :param X_true\n",
    "        attributes of the groung truth (training set)\n",
    "    :param y_true\n",
    "        classes of the groung truth (training set)\n",
    "    :param X_pred\n",
    "        attributes of samples to be classified\n",
    "    :param k\n",
    "        number of neighbors to use\n",
    "    :return\n",
    "        predicted classes\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ### \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = knn(X_train, y_train, X_test, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ten predictions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now we would like to assess how well our classifier performs.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Implement a function for calculating the accuracy of your predictions given the ground truth and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function calculating the accuracy of the model on the given data.\n",
    "    \n",
    "    :param y_true\n",
    "        true classes\n",
    "    :paaram y\n",
    "        predicted classes\n",
    "    :return\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ### \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(y_test, y_hat)\n",
    "print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many items where misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('misclassified =', sum(abs(y_hat - y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How balanced is our test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's balanced, we don't have to be worried about objectivity of the accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Congratulations! At this point, hopefully, you have successufuly implemented a k-nearest neighbors algorithm able to classify unseen samples with high accuracy.\n",
    "\n",
    "✌️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
